# ğŸ¯ Clickbait Spoiling Project

## ğŸ“‹ Tá»•ng quan

Dá»± Ã¡n nÃ y nghiÃªn cá»©u vá» **clickbait spoiling** - viá»‡c táº¡o ra cÃ¡c Ä‘oáº¡n vÄƒn báº£n ngáº¯n gá»n nháº±m thá»a mÃ£n sá»± tÃ² mÃ² Ä‘Æ°á»£c táº¡o ra bá»Ÿi cÃ¡c tiÃªu Ä‘á» clickbait. Dá»± Ã¡n sá»­ dá»¥ng dataset **Webis Clickbait Spoiling Corpus 2022** vá»›i 5,000 bÃ i viáº¿t clickbait Ä‘Æ°á»£c thu tháº­p tá»« Facebook, Reddit vÃ  Twitter.

## ğŸ¯ Má»¥c tiÃªu

- ğŸ¤– XÃ¢y dá»±ng cÃ¡c mÃ´ hÃ¬nh AI Ä‘á»ƒ tá»± Ä‘á»™ng táº¡o ra spoiler cho cÃ¡c tiÃªu Ä‘á» clickbait
- ğŸ“Š So sÃ¡nh hiá»‡u suáº¥t cá»§a cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau: rule-based, classical ML, vÃ  deep learning
- ğŸ“ˆ ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng spoiler báº±ng nhiá»u metrics khÃ¡c nhau (BLEU, ROUGE, BERTScore)
- ğŸš€ Triá»ƒn khai mÃ´ hÃ¬nh tá»‘t nháº¥t thÃ nh á»©ng dá»¥ng demo

## ğŸ“Š Dataset

### Thá»‘ng kÃª Dataset
- **Training set**: 3,200 samples  
- **Validation set**: 800 samples
- **Test set**: 1,000 samples

### Loáº¡i Spoiler
- **Phrase spoilers**: CÃ¢u tráº£ lá»i ngáº¯n gá»n (1,367 samples trong táº­p train)
- **Passage spoilers**: Äoáº¡n vÄƒn báº£n dÃ i hÆ¡n (1,274 samples trong táº­p train)  
- **Multi spoilers**: Nhiá»u pháº§n khÃ´ng liÃªn tá»¥c (559 samples trong táº­p train)

### Ná»n táº£ng
- **Twitter**: 1,530 samples (train)
- **Reddit**: 1,150 samples (train)
- **Facebook**: 520 samples (train)

## ğŸ› ï¸ MÃ´i trÆ°á»ng phÃ¡t triá»ƒn

### YÃªu cáº§u há»‡ thá»‘ng
- Python >= 3.7 (Ä‘ang sá»­ dá»¥ng Python 3.10.18)
- Conda environment: `clickbait`
- CUDA support (tÃ¹y chá»n cho GPU acceleration)

### ThÆ° viá»‡n chÃ­nh
```bash
# Core ML/DL libraries
torch>=1.9.0
transformers>=4.20.0  
sentence-transformers>=2.2.0
scikit-learn>=1.0.0

# Natural Language Processing
nltk>=3.7
spacy>=3.4.0

# Evaluation metrics
sacrebleu>=2.0.0
rouge-score>=0.1.0
bert-score>=0.3.0

# Data processing
pandas>=1.3.0
numpy>=1.21.0
datasets>=2.0.0
```

## ğŸš€ CÃ i Ä‘áº·t

1. **KÃ­ch hoáº¡t mÃ´i trÆ°á»ng conda**:
```bash
conda activate clickbait
```

2. **CÃ i Ä‘áº·t dependencies**:
```bash
pip install -r requirements.txt
```

3. **Táº£i spaCy model**:
```bash
python -m spacy download en_core_web_sm
```

4. **Thiáº¿t láº­p NLTK data**:
```bash
python scripts/setup_nltk.py
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
clickbait-spoiling/
â”œâ”€â”€ data/                    # Raw dataset
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ train.jsonl         # Training data (3,200 samples)
â”‚   â”œâ”€â”€ validation.jsonl    # Validation data (800 samples)
â”‚   â””â”€â”€ test.jsonl         # Test data (1,000 samples)
â”œâ”€â”€ processed_data/         # Preprocessed data
â”‚   â”œâ”€â”€ train_generation.jsonl
â”‚   â”œâ”€â”€ validation_generation.jsonl
â”‚   â”œâ”€â”€ test_generation.jsonl
â”‚   â”œâ”€â”€ train_classification.pkl
â”‚   â”œâ”€â”€ validation_classification.pkl
â”‚   â”œâ”€â”€ test_classification.pkl
â”‚   â””â”€â”€ preprocessing_report.md
â”œâ”€â”€ scripts/                # Utility scripts
â”‚   â”œâ”€â”€ setup_nltk.py      # Setup NLTK data
â”‚   â”œâ”€â”€ data_exploration.py # Data exploration and analysis
â”‚   â”œâ”€â”€ data_preprocessor.py # Data preprocessing
â”‚   â”œâ”€â”€ evaluator.py       # Model evaluation
â”‚   â””â”€â”€ verify_environment.py # Environment verification
â”œâ”€â”€ evaluation_results/     # Model evaluation outputs
â”œâ”€â”€ models/                 # Trained models (to be created)
â”œâ”€â”€ notebooks/             # Jupyter notebooks (to be created)
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ preprocess_data.py     # Main preprocessing script
â”œâ”€â”€ train_gpt2_spoiler.py  # GPT-2 training script
â”œâ”€â”€ test_gpt2_model.py     # GPT-2 testing script
â””â”€â”€ README.md             # This file
```

## ğŸ” KhÃ¡m phÃ¡ vÃ  xá»­ lÃ½ dá»¯ liá»‡u

### 1. KhÃ¡m phÃ¡ dá»¯ liá»‡u
```bash
python scripts/data_exploration.py
```

Script nÃ y sáº½:
- ğŸ“Š PhÃ¢n tÃ­ch cáº¥u trÃºc dataset
- ğŸ“ˆ Táº¡o visualizations vá» phÃ¢n phá»‘i dá»¯ liá»‡u
- ğŸ’¡ Hiá»ƒn thá»‹ vÃ­ dá»¥ cho tá»«ng loáº¡i spoiler
- ğŸ’¾ LÆ°u biá»ƒu Ä‘á»“ phÃ¢n tÃ­ch vÃ o `data_analysis.png`

### 2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
```bash
python preprocess_data.py
```

Script nÃ y sáº½:
- ğŸ§¹ LÃ m sáº¡ch vÃ  chuáº©n hÃ³a text
- ğŸ”¤ Tokenization vá»›i GPT-2 tokenizer
- ğŸ’¾ LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ vÃ o `processed_data/`
- ğŸ“‹ Táº¡o bÃ¡o cÃ¡o preprocessing

## ğŸ“ˆ PhÃ¢n tÃ­ch Dataset

### Äá»™ dÃ i Spoiler
- **Trung bÃ¬nh**: 84.6 kÃ½ tá»± (train set)
- **Trung vá»‹**: 43.0 kÃ½ tá»± (train set)
- **Pháº¡m vi**: 2-1,369 kÃ½ tá»±

### VÃ­ dá»¥ theo loáº¡i Spoiler

**Phrase Spoilers**:
- Post: "NASA sets date for full recovery of ozone hole"
- Spoiler: "2070"

**Passage Spoilers**:
- Post: "What happens if your new AirPods get lost or stolen, will Apple do anything?"
- Spoiler: "Apple says that if AirPods are lost or stolen, you'll have to buy new ones, just like any other Apple product."

**Multi Spoilers**:
- Post: "Hot Sauce Taste Test: Find out which we named number 1"
- Spoiler: "Sriracha Hot Chili Sauce"

## ğŸš€ Huáº¥n luyá»‡n vÃ  Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh

### 1. Huáº¥n luyá»‡n mÃ´ hÃ¬nh GPT-2
```bash
python train_gpt2_spoiler.py
```

### 2. Kiá»ƒm tra mÃ´ hÃ¬nh
```bash
python test_gpt2_model.py
```

### 3. ÄÃ¡nh giÃ¡ chi tiáº¿t
```bash
python scripts/evaluator.py
```

## ğŸ“Š Káº¿t quáº£ hiá»‡n táº¡i

### Tiáº¿n Ä‘á»™ dá»± Ã¡n
- âœ… **Data Exploration**: HoÃ n thÃ nh phÃ¢n tÃ­ch dataset
- âœ… **Data Preprocessing**: ÄÃ£ xá»­ lÃ½ vÃ  tokenize dá»¯ liá»‡u
- âœ… **GPT-2 Implementation**: ÄÃ£ triá»ƒn khai mÃ´ hÃ¬nh GPT-2 cÆ¡ báº£n
- ğŸ”„ **Model Training**: Äang trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n
- â³ **Evaluation**: Chá» hoÃ n thÃ nh training
- â³ **Deployment**: Káº¿ hoáº¡ch tÆ°Æ¡ng lai

### Thá»‘ng kÃª dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
- **Training Generation**: 52.2 MB (3,200 samples)
- **Validation Generation**: 13.3 MB (800 samples)
- **Test Generation**: 16.2 MB (1,000 samples)
- **Classification Data**: ÄÃ£ tokenize vÃ  lÆ°u dáº¡ng pickle

## ğŸ¯ Káº¿ hoáº¡ch tiáº¿p theo

1. **Model Optimization**: Tinh chá»‰nh hyperparameters
2. **Advanced Models**: Thá»­ nghiá»‡m BERT, T5, vÃ  cÃ¡c mÃ´ hÃ¬nh khÃ¡c
3. **Ensemble Methods**: Káº¿t há»£p nhiá»u mÃ´ hÃ¬nh
4. **Web Demo**: Táº¡o giao diá»‡n web Ä‘á»ƒ demo
5. **Performance Analysis**: PhÃ¢n tÃ­ch chi tiáº¿t káº¿t quáº£

## ğŸ“š Tham kháº£o

- [Webis Clickbait Spoiling Corpus 2022](https://webis.de/data/webis-clickbait-22.html)
- [Hugging Face Transformers](https://huggingface.co/transformers/)
- [Sentence Transformers](https://www.sbert.net/)

## ğŸ› ï¸ Troubleshooting

### Lá»—i thÆ°á»ng gáº·p

1. **CUDA out of memory**: Giáº£m batch size trong training script
2. **NLTK data missing**: Cháº¡y `python scripts/setup_nltk.py`
3. **spaCy model missing**: Cháº¡y `python -m spacy download en_core_web_sm`
4. **Import errors**: Kiá»ƒm tra environment vá»›i `python scripts/verify_environment.py`

### Kiá»ƒm tra mÃ´i trÆ°á»ng
```bash
python scripts/verify_environment.py
```

## ğŸ‘¥ Contributors

- **Sinh viÃªn**: [TÃªn cá»§a báº¡n]
- **TrÆ°á»ng**: Äáº¡i há»c Ngoáº¡i ngá»¯ - Tin há»c TP.HCM (HUFLIT)
- **KhÃ³a luáº­n tá»‘t nghiá»‡p**: Clickbait Spoiling vá»›i Deep Learning

## ğŸ“„ License

MIT License - Xem file LICENSE Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t

## ğŸ“ LiÃªn há»‡

- ğŸ“§ Email: [your-email@example.com]
- ğŸ™ GitHub: [your-github-username]
- ğŸ“± LinkedIn: [your-linkedin-profile]
